{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "890ea3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPCA: C:\\Users\\fdani\\project_ipca_boi\\lakehouse\\bronze\\ipca\n",
      "BOI : C:\\Users\\fdani\\project_ipca_boi\\lakehouse\\bronze\\boi_gordo\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# PROJECT_ROOT = Path().resolve()  # se o notebook estiver em /notebooks, pode ajustar abaixo\n",
    "PROJECT_ROOT = Path().resolve().parents[0]  # (se necessário)\n",
    "\n",
    "IPCA_PATH = (PROJECT_ROOT / \"lakehouse\" / \"bronze\" / \"ipca\").resolve()\n",
    "BOI_PATH  = (PROJECT_ROOT / \"lakehouse\" / \"bronze\" / \"boi_gordo\").resolve()\n",
    "\n",
    "print(\"IPCA:\", IPCA_PATH)\n",
    "print(\"BOI :\", BOI_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70c4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import os, sys\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# Spark Session (igual aos pipelines Bronze)\n",
    "# --------------------------------------------\n",
    "\n",
    "def get_spark(app_name: str = \"validation_bronze\") -> SparkSession:\n",
    "    \n",
    "    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "    builder = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "        .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\")  # bom para local\n",
    "    )\n",
    "    return configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7536285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abertura da sessão:\n",
    "\n",
    "spark = get_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8a07416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+---------+------------+----------+--------------------+--------------------+--------------------+--------------------+----+---+\n",
      "|  data_ref|valor| source|series_id|data_inicial|data_final|         request_url|        payload_hash|       ingest_run_id|     ingested_at_utc| ano|mes|\n",
      "+----------+-----+-------+---------+------------+----------+--------------------+--------------------+--------------------+--------------------+----+---+\n",
      "|2025-02-01| 1.31|BCB_SGS|      433|  01/01/2025|31/12/2025|https://api.bcb.g...|edb41f451ba121bbe...|1796618f-301f-47a...|2026-02-06 01:52:...|2025|  2|\n",
      "|2025-06-01| 0.24|BCB_SGS|      433|  01/01/2025|31/12/2025|https://api.bcb.g...|edb41f451ba121bbe...|1796618f-301f-47a...|2026-02-06 01:52:...|2025|  6|\n",
      "|2025-05-01| 0.26|BCB_SGS|      433|  01/01/2025|31/12/2025|https://api.bcb.g...|edb41f451ba121bbe...|1796618f-301f-47a...|2026-02-06 01:52:...|2025|  5|\n",
      "|2025-12-01| 0.33|BCB_SGS|      433|  01/01/2025|31/12/2025|https://api.bcb.g...|edb41f451ba121bbe...|1796618f-301f-47a...|2026-02-06 01:52:...|2025| 12|\n",
      "|2025-09-01| 0.48|BCB_SGS|      433|  01/01/2025|31/12/2025|https://api.bcb.g...|edb41f451ba121bbe...|1796618f-301f-47a...|2026-02-06 01:52:...|2025|  9|\n",
      "+----------+-----+-------+---------+------------+----------+--------------------+--------------------+--------------------+--------------------+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leitura do IPCA do ano de 2025:\n",
    "(\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .load(str(IPCA_PATH))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d75c31fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+--------------------+--------------------+-------------------+--------------------+--------------------+---+----+\n",
      "|            csv_file|mes_ano| valor|         source_file|           file_hash|   converted_at_utc|       ingest_run_id|     ingested_at_utc|mes| ano|\n",
      "+--------------------+-------+------+--------------------+--------------------+-------------------+--------------------+--------------------+---+----+\n",
      "|cepea-consulta-20...|03/2025|312.47|cepea-consulta-20...|3f0d5e276f235d8d2...|2026-02-05 23:20:02|5092bde9-7049-435...|2026-02-06 00:45:...|  3|2025|\n",
      "|cepea-consulta-20...|07/2025|299.97|cepea-consulta-20...|3f0d5e276f235d8d2...|2026-02-05 23:20:02|5092bde9-7049-435...|2026-02-06 00:45:...|  7|2025|\n",
      "|cepea-consulta-20...|10/2025|310.51|cepea-consulta-20...|3f0d5e276f235d8d2...|2026-02-05 23:20:02|5092bde9-7049-435...|2026-02-06 00:45:...| 10|2025|\n",
      "|cepea-consulta-20...|04/2025|323.96|cepea-consulta-20...|3f0d5e276f235d8d2...|2026-02-05 23:20:02|5092bde9-7049-435...|2026-02-06 00:45:...|  4|2025|\n",
      "|cepea-consulta-20...|01/2025|324.95|cepea-consulta-20...|3f0d5e276f235d8d2...|2026-02-05 23:20:02|5092bde9-7049-435...|2026-02-06 00:45:...|  1|2025|\n",
      "+--------------------+-------+------+--------------------+--------------------+-------------------+--------------------+--------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leitura do boi_gordo de 2025:\n",
    "(\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .load(str(BOI_PATH))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90be1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IPCA: schema ===\n",
      "root\n",
      " |-- data_ref: date (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- series_id: integer (nullable = true)\n",
      " |-- data_inicial: string (nullable = true)\n",
      " |-- data_final: string (nullable = true)\n",
      " |-- request_url: string (nullable = true)\n",
      " |-- payload_hash: string (nullable = true)\n",
      " |-- ingest_run_id: string (nullable = true)\n",
      " |-- ingested_at_utc: timestamp (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      "\n",
      "=== IPCA: sample ===\n",
      "+----------+-----+-------+---------+------------+----------+----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+------------------------------------+--------------------------+----+---+\n",
      "|data_ref  |valor|source |series_id|data_inicial|data_final|request_url                                                                                                     |payload_hash                                                    |ingest_run_id                       |ingested_at_utc           |ano |mes|\n",
      "+----------+-----+-------+---------+------------+----------+----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+------------------------------------+--------------------------+----+---+\n",
      "|2025-02-01|1.31 |BCB_SGS|433      |01/01/2025  |31/12/2025|https://api.bcb.gov.br/dados/serie/bcdata.sgs.433/dados?dataInicial=01/01/2025&dataFinal=31/12/2025&formato=json|edb41f451ba121bbee8af8d90493a237d05968ca8a654da4568d523603050761|1796618f-301f-47ab-82a3-caf46c8e0b96|2026-02-06 01:52:55.954725|2025|2  |\n",
      "|2025-06-01|0.24 |BCB_SGS|433      |01/01/2025  |31/12/2025|https://api.bcb.gov.br/dados/serie/bcdata.sgs.433/dados?dataInicial=01/01/2025&dataFinal=31/12/2025&formato=json|edb41f451ba121bbee8af8d90493a237d05968ca8a654da4568d523603050761|1796618f-301f-47ab-82a3-caf46c8e0b96|2026-02-06 01:52:55.954725|2025|6  |\n",
      "|2025-05-01|0.26 |BCB_SGS|433      |01/01/2025  |31/12/2025|https://api.bcb.gov.br/dados/serie/bcdata.sgs.433/dados?dataInicial=01/01/2025&dataFinal=31/12/2025&formato=json|edb41f451ba121bbee8af8d90493a237d05968ca8a654da4568d523603050761|1796618f-301f-47ab-82a3-caf46c8e0b96|2026-02-06 01:52:55.954725|2025|5  |\n",
      "|2025-12-01|0.33 |BCB_SGS|433      |01/01/2025  |31/12/2025|https://api.bcb.gov.br/dados/serie/bcdata.sgs.433/dados?dataInicial=01/01/2025&dataFinal=31/12/2025&formato=json|edb41f451ba121bbee8af8d90493a237d05968ca8a654da4568d523603050761|1796618f-301f-47ab-82a3-caf46c8e0b96|2026-02-06 01:52:55.954725|2025|12 |\n",
      "|2025-09-01|0.48 |BCB_SGS|433      |01/01/2025  |31/12/2025|https://api.bcb.gov.br/dados/serie/bcdata.sgs.433/dados?dataInicial=01/01/2025&dataFinal=31/12/2025&formato=json|edb41f451ba121bbee8af8d90493a237d05968ca8a654da4568d523603050761|1796618f-301f-47ab-82a3-caf46c8e0b96|2026-02-06 01:52:55.954725|2025|9  |\n",
      "+----------+-----+-------+---------+------------+----------+----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+------------------------------------+--------------------------+----+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Validando schema: IPCA ===\n",
      " Colunas faltando: ['data', 'ipca_mensal', 'ipca_acumulado_12m', 'source_file', 'ingestion_ts']\n",
      " Colunas extras: ['data_ref', 'valor', 'source', 'series_id', 'data_inicial', 'data_final', 'request_url', 'payload_hash', 'ingest_run_id', 'ingested_at_utc']\n"
     ]
    }
   ],
   "source": [
    "# validações de estrutura de IPCA:\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    DateType, TimestampType, StringType, IntegerType,\n",
    "    DoubleType\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Leitura Delta (cria df_ipca)\n",
    "# -----------------------\n",
    "df_ipca = (\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .load(str(IPCA_PATH))\n",
    ")\n",
    "\n",
    "# (Opcional) ver schema e amostra\n",
    "print(\"=== IPCA: schema ===\")\n",
    "df_ipca.printSchema()\n",
    "print(\"=== IPCA: sample ===\")\n",
    "df_ipca.show(5, truncate=False)\n",
    "\n",
    "# -----------------------\n",
    "# Validação de Schema / Tipos\n",
    "# -----------------------\n",
    "def validate_schema(df, expected: dict, df_name: str = \"df\"):\n",
    "    schema = {f.name: type(f.dataType) for f in df.schema.fields}\n",
    "\n",
    "    missing = [c for c in expected.keys() if c not in schema]\n",
    "    extra   = [c for c in schema.keys() if c not in expected]\n",
    "    wrong   = [\n",
    "        (c, schema[c].__name__, expected[c].__name__)\n",
    "        for c in expected.keys()\n",
    "        if c in schema and schema[c] != expected[c]\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n=== Validando schema: {df_name} ===\")\n",
    "    if missing: print(\" Colunas faltando:\", missing)\n",
    "    if extra:   print(\" Colunas extras:\", extra)\n",
    "    if wrong:\n",
    "        print(\" Tipos divergentes:\")\n",
    "        for c, got, exp in wrong:\n",
    "            print(f\"   - {c}: atual={got} esperado={exp}\")\n",
    "\n",
    "    if not missing and not wrong:\n",
    "        print(\" Schema OK (colunas e tipos conferem)\")\n",
    "\n",
    "# Ajuste para o schema real :\n",
    "expected_ipca = {\n",
    "    \"data\": DateType,\n",
    "    \"ano\": IntegerType,\n",
    "    \"mes\": IntegerType,\n",
    "    \"ipca_mensal\": DoubleType,\n",
    "    \"ipca_acumulado_12m\": DoubleType,\n",
    "    \"source_file\": StringType,\n",
    "    \"ingestion_ts\": TimestampType\n",
    "}\n",
    "\n",
    "validate_schema(df_ipca, expected_ipca, df_name=\"IPCA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ffd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BOI: schema ===\n",
      "root\n",
      " |-- csv_file: string (nullable = true)\n",
      " |-- mes_ano: string (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      " |-- file_hash: string (nullable = true)\n",
      " |-- converted_at_utc: timestamp (nullable = true)\n",
      " |-- ingest_run_id: string (nullable = true)\n",
      " |-- ingested_at_utc: timestamp (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      "\n",
      "=== BOI: sample ===\n",
      "+---------------------------------+-------+------+---------------------------------+----------------------------------------------------------------+-------------------+------------------------------------+--------------------------+---+----+\n",
      "|csv_file                         |mes_ano|valor |source_file                      |file_hash                                                       |converted_at_utc   |ingest_run_id                       |ingested_at_utc           |mes|ano |\n",
      "+---------------------------------+-------+------+---------------------------------+----------------------------------------------------------------+-------------------+------------------------------------+--------------------------+---+----+\n",
      "|cepea-consulta-20260131203157.csv|03/2025|312.47|cepea-consulta-20260131203157.xls|3f0d5e276f235d8d2e7a49218ea4fbc11d219de76083e99f75ef2a935bca1d09|2026-02-05 23:20:02|5092bde9-7049-4356-96b7-eaa42cac86d0|2026-02-06 00:45:53.029236|3  |2025|\n",
      "|cepea-consulta-20260131203157.csv|07/2025|299.97|cepea-consulta-20260131203157.xls|3f0d5e276f235d8d2e7a49218ea4fbc11d219de76083e99f75ef2a935bca1d09|2026-02-05 23:20:02|5092bde9-7049-4356-96b7-eaa42cac86d0|2026-02-06 00:45:53.029236|7  |2025|\n",
      "|cepea-consulta-20260131203157.csv|10/2025|310.51|cepea-consulta-20260131203157.xls|3f0d5e276f235d8d2e7a49218ea4fbc11d219de76083e99f75ef2a935bca1d09|2026-02-05 23:20:02|5092bde9-7049-4356-96b7-eaa42cac86d0|2026-02-06 00:45:53.029236|10 |2025|\n",
      "|cepea-consulta-20260131203157.csv|04/2025|323.96|cepea-consulta-20260131203157.xls|3f0d5e276f235d8d2e7a49218ea4fbc11d219de76083e99f75ef2a935bca1d09|2026-02-05 23:20:02|5092bde9-7049-4356-96b7-eaa42cac86d0|2026-02-06 00:45:53.029236|4  |2025|\n",
      "|cepea-consulta-20260131203157.csv|01/2025|324.95|cepea-consulta-20260131203157.xls|3f0d5e276f235d8d2e7a49218ea4fbc11d219de76083e99f75ef2a935bca1d09|2026-02-05 23:20:02|5092bde9-7049-4356-96b7-eaa42cac86d0|2026-02-06 00:45:53.029236|1  |2025|\n",
      "+---------------------------------+-------+------+---------------------------------+----------------------------------------------------------------+-------------------+------------------------------------+--------------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Validando schema: BOI ===\n",
      " Colunas faltando: ['data', 'preco', 'ingestion_ts']\n",
      " Colunas extras: ['csv_file', 'mes_ano', 'valor', 'file_hash', 'converted_at_utc', 'ingest_run_id', 'ingested_at_utc', 'mes', 'ano']\n"
     ]
    }
   ],
   "source": [
    "# validações de estrutura de BOI_GORDO:\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    DateType, TimestampType, StringType, IntegerType,\n",
    "    DoubleType\n",
    ")\n",
    "\n",
    "df_boi = (\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .load(str(BOI_PATH))\n",
    ")\n",
    "\n",
    "print(\"=== BOI: schema ===\")\n",
    "df_boi.printSchema()\n",
    "print(\"=== BOI: sample ===\")\n",
    "df_boi.show(5, truncate=False)\n",
    "\n",
    "# Exemplo (ajuste pro schema real)\n",
    "expected_boi = {\n",
    "    \"data\": DateType,\n",
    "    \"preco\": DoubleType,\n",
    "    \"source_file\": StringType,\n",
    "    \"ingestion_ts\": TimestampType\n",
    "}\n",
    "\n",
    "validate_schema(df_boi, expected_boi, df_name=\"BOI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9363d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9f52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|bronze   |\n",
      "|default  |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criar databases “bronze”:\n",
    "# Isso faz o “schema/database” do catálogo ter um uma base de dados lógico DELTA LAKE:\n",
    "\n",
    "BRONZE_DB_PATH = (PROJECT_ROOT / \"lakehouse\" / \"bronze\").as_posix()\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS bronze LOCATION '{BRONZE_DB_PATH}'\")\n",
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27249dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|bronze   |boi_gordo|false      |\n",
      "|bronze   |ipca     |false      |\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registrar tabelas Bronze existentes (que hoje só são “paths”):\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bronze.ipca\n",
    "USING DELTA\n",
    "LOCATION '{IPCA_PATH.as_posix()}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bronze.boi_gordo\n",
    "USING DELTA\n",
    "LOCATION '{BOI_PATH.as_posix()}'\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab6a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|bronze   |boi_gordo|false      |\n",
      "|bronze   |ipca     |false      |\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A partir daqui você pode carregar e testar agora:\n",
    "\n",
    "spark.sql(\"SHOW TABLES IN bronze\").show(truncate=False)\n",
    "spark.table(\"bronze.ipca\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf9ccdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ipca_boi_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
